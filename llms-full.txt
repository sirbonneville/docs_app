# https://beta.docs.pieces.app/api-reference/apireference

# https://beta.docs.pieces.app/api-reference/apireference



---

# Meet Pieces: AI-Driven Dev Efficiency

# Meet Pieces: AI-Driven Dev Efficiency

Getting to Know Pieces
======================

---

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734014941536/9c298363-d6b4-4e4a-9965-72db0fa59bc6.png?auto=compress,format&format=webp&q=75)

---

Pieces is an **AI-enabled productivity tool** designed to **increase developer efficiency** through **personalized workflow assistance** across the entire toolchain, featuring a dedicated desktop application and integrations with many popular IDEs.

---

Introducing Pieces for Developers
---------------------------------

This documentation is designed to provide a high-level overview of [Pieces features and functionalities](https://beta.docs.pieces.app/products/meet-pieces/fundamentals), with a quick-start and troubleshooting guides for [macOS](https://beta.docs.pieces.app/products/meet-pieces/get-started/macos), [Windows](https://beta.docs.pieces.app/products/meet-pieces/get-started/windows), and [Linux](https://beta.docs.pieces.app/products/meet-pieces/get-started/linux) to help you get started with Pieces as quick as possible.

Let’s answer a few basic questions:

### What is Pieces?

At its core, Pieces is an all-in-one productivity tool that helps developers work *smarter*, not harder, by anchoring engineers to their workflow with the power of the **Long-Term Memory Engine, Pieces Drive,** and the **Pieces Copilot.**

There are **[3]** core pillars of Pieces functionality:

1. [Long-Term Memory Engine (LTM-2)](https://beta.docs.pieces.app/products/meet-pieces/fundamentals#ltm-2): A powerful, AI-powered live context framework that understands what you’re working on across your entire development workflow. The LTM-2 helps by outsourcing the burden of memory and retrieval from the developer to AI to maximize space for creativity driven by human intent.
2. [Pieces Drive](https://beta.docs.pieces.app/products/meet-pieces/fundamentals#pieces-drive): The ability to save, search, reference, reuse, and share small developer resources like code snippets, screenshots, links, and text notes.
3. [Pieces Copilot](https://beta.docs.pieces.app/products/meet-pieces/fundamentals#pieces-copilot): An intelligent assistant that helps with generating code, answering questions, and adding code comments while utilizing a LLM of your choice—featuring an adjustable context window ranging from conversation-only to entire project repositories.

### Who is Pieces For?

To put it simply, Pieces is built from the ground up for developers who frequently reference or reuse small developer materials like code snippets or are looking to pick up where they left off and preserve the context of their workflow.

Pieces is an **excellent productivity option for developers of all levels** and across various fields, including front-end developers, data scientists, DevOps engineers, and students.

If you find yourself …

1. **Preserving Workflow Context:** If you’re wondering where you left off with trying to untangle that permissions mess in Google Cloud Console or Firestore database, the [LTM-2](https://beta.docs.pieces.app/products/meet-pieces/fundamentals#ltm-2) is an invaluable resource that can provide deep-links and anchors in the form of URLs and other helpful context.
2. **Managing Developer Materials**: If you find yourself needing to repurpose or refresh your memory with small developer resources throughout your workflow, the [Pieces Drive](https://beta.docs.pieces.app/products/meet-pieces/fundamentals#pieces-drive) can help you stay organized while providing an efficient pipeline for referencing and reusing them.
3. **Needing Code Assistance:** Stuck on a bug, or don’t know how to solve a complex algorithmic efficiency problem? Can’t center that `div`? Ask [Pieces Copilot](https://beta.docs.pieces.app/products/meet-pieces/fundamentals#pieces-copilot) to help you understand your function, script, code file or entire project and generate code to insert directly into your active file.

… then Pieces is intelligently designed with *you,* the developer, in mind.

Let Pieces do the heavy lifting and preserve your engineering creativity and ingenuity for where you need it most.

### How do I Use Pieces?

PiecesOS serves as the foundational layer of the entire Pieces for Developers Suite, much like Docker does for containerized applications—it powers the Pieces Desktop App along with all our browser and IDE extensions and plugins.

By installing PiecesOS, you enable the full functionality of the desktop app and our extensions, just like how Docker manages various containers.

We designed it this way to ensure that your data—including machine learning processes and inter-app communications—remains on-device, secure, private, and local.

Click one of the links below to fast-forward to the Pieces Suite installation guide for your operating system:

* [macOS](https://beta.docs.pieces.app/products/meet-pieces/macos-quickstart)
* [Windows](https://beta.docs.pieces.app/products/meet-pieces/windows-quickstart)
* [Linux](https://beta.docs.pieces.app/products/meet-pieces/linux-quickstart)

---

PiecesOS is required for any Pieces Software to work, including the Pieces Desktop App. However, the Pieces Desktop App is not itself required—but it is recommended to install both PiecesOS and the Pieces Desktop App.

---

Updated on March 5, 2025, 9:13 PM UTC

---

[🧠 Fundamentals](/products/meet-pieces/fundamentals)

---

# Pieces for Developers

# Pieces for Developers

---

Ollama | Manual Installation
============================

Read documentation on manually installing Ollama outside of the Pieces Desktop App for use with PiecesOS.

---

Manual Installation Guide
-------------------------

Ollama is an *optional* dependency that enables local AI inference for Pieces Copilot and other AI-powered features in Pieces for Developers.

If you prefer to run LLMs on-device instead of using cloud-based AI, you will need to install Ollama manually.

This guide will walk you through the download, installation, and verification process for Ollama across Windows, macOS, and Linux.

### Minimum Version Requirement

PiecesOS requires a **specific minimum version** of Ollama to ensure compatibility, which is **0.5.5.**

Always install the latest stable release.

### Download Ollama

Visit the [official Ollama website](https://ollama.com/download) to download the latest version, then install using the platform-specific instructions below.

macOS

Windows

Linux

**Ollama | macOS Installation**

Follow the steps below to manually install Ollama for your macOS device.

1. Download the **macOS installer (**`.pkg`**)** from the [Ollama website.](https://ollama.com/download)
2. Double-click the `.pkg` file and follow the installation steps.
3. After installation, verify the setup in **Terminal** by running `ollama --version`.

---

### Verify Ollama Integration

Once installed, ensure PiecesOS can detect and use Ollama.

1. Open the **Pieces Quick Menu** from your system tray or menu bar.
2. Navigate to `ML Processing`.
3. If Ollama is installed and recognized, it will appear under `Local AI Models`.

If it does not appear, restart PiecesOS and try again.

If PiecesOS **still doesn’t detect Ollama**, refer to [troubleshooting.](https://beta.docs.pieces.app/products/core-dependencies/ollama/troubleshooting)

### Update Ollama

To update or uninstall Ollama on macOS or Windows, you can either download the latest version from the official Ollama website, update it through the Pieces Desktop App (if installed) or update it directly from the background Ollama process on your device.

For Linux, open your terminal and run `sudo apt update && sudo apt upgrade ollama`.

### Uninstall Ollama

If you no longer need local AI models or wish to remove the Ollama wrapper from your system, follow the instructions below specific to your platform.

---

macOS

Windows

Linux

**Ollama | macOS Uninstallation**

1. Open your terminal and run `sudo rm -rf /[username]/local/bin/ollama`.

---

### Next Steps

You can read documentation about what [local LLMs are currently available](https://beta.docs.pieces.app/products/core-dependencies/ollama/supported-models) on Ollama and are supported by PiecesOS, or [click here for troubleshooting](http://beta.docs.pieces.app/products/core-dependencies/ollama/troubleshooting) if you’re experiencing installation issue.

Updated on March 5, 2025, 9:13 PM UTC

---

[Ollama](/products/core-dependencies/ollama)

[Supported Models](/products/core-dependencies/ollama/supported-models)

---

# Pieces for Developers

# Pieces for Developers

---

Available Local LLMs
====================

Find reference information and an up-to-date (January 31st, 2025) of local LLMs available for download that are currently supported by PiecesOS, the Pieces Desktop App, and other Pieces plugins and extensions.

---

Supported LLMs
--------------

The Pieces for Developers Suite currently supports 41 local models from a range of providers.

---

| **Provider** | **Model Name** |
| --- | --- |
| *Google* | Gemma / Code Gemma |
| --- | --- |
| *IBM* | Granite / Code / Dense / MoE |
| *Meta* | LLaMA / CodeLLaMA |
| *Mistral* | Mistral / Mixtral |
| *Microsoft* | Phi |
| *Qwen* | QwQ / Coder |
| *StarCoder* | StarCoder |

View the tables below for detailed model names, parameters, and the context window size of all usable models.

---

Please note that not all specific models have easily indentifiable **parameter quantities**. Some companies release information on their models, while others do not—as such, the parameters provided in these tables are **estimated parameter ranges** based on leading AI sources, detailed evaluations and assessments, and other available information.

---

### Google (Gemma)

---

| **Model Name** | **Parameters** | **Context Window (Maximum)** |
| --- | --- | --- |
| *Gemma 2 27B* | 27B | 8k tokens |
| --- | --- | --- |
| *Gemma 2 9B* | 9B | 8k tokens |
| *Gemma 2 2B* | 2B | 8k tokens |
| *Gemma 1.1 7B* | 7B | 4k tokens |
| *Gemma 1.1 2B* | 2B | 4k tokens |
| *Code Gemma 1.1 7B* | 7B | 4k tokens |

### IBM (Granite)

---

| **Model Name** | **Parameters** | **Context Window (Maximum)** |
| --- | --- | --- |
| *Granite Code 34B* | 34B | 8k tokens |
| --- | --- | --- |
| *Granite Code 20B* | 20B | 8k tokens |
| *Granite Code 8B* | 8B | 128k tokens |
| *Granite Code 3B 128K* | 3B | 128k tokens |
| *Granite Code 3B* | 3B | 4k tokens |
| *Granite 3.1 Dense 8B* | 8B | 128k tokens |
| *Granite 3.1 Dense 2B* | 2B | 128k tokens |
| *Granite 3 MoE 3B* | 3B | 128k tokens |
| *Granite 3 MoE 1B* | 1B | 128k tokens |
| *Granite 3 Dense 8B* | 8B | 128k tokens |

### Meta (LLaMA)

---

| **Model Name** | **Parameters** | **Context Window (Maximum)** |
| --- | --- | --- |
| *LLaMA 3.2 3B* | 3B | 128k tokens |
| --- | --- | --- |
| *LLaMA 3.2 1B* | 1B | 8k tokens |
| *LLaMA 3 8B* | 8B | 8k tokens |
| *LLaMA 2 13B* | 13B | 4lk tokens |
| *LLaMA 2 7B* | 7B | 4k tokens |
| *CodeLLaMA 34B* | 34B | 100k tokens |
| *CodeLLaMA 13B* | 13B | 16k tokens |
| *CodeLLaMA 7B* | 7B | 8k tokens |

### Mistral (Mixtral)

---

| **Model Name** | **Parameters** | **Context Window (Maximum)** |
| --- | --- | --- |
| *Mixtral 8 7B* | 7B | 128k tokens |
| --- | --- | --- |
| *Mistral 7B* | 7B | 32.8k tokens |

### Microsoft (Phi)

---

| **Model Name** | **Parameters** | **Context Window** |
| --- | --- | --- |
| *Phi-4 14B* | 14B | 4k tokens |
| --- | --- | --- |
| *Phi-3.5 Mini 3.8B* | 3.8B | 128k tokens |
| *Phi-3 Mini 128K* | 3B | 128k tokens |
| *Phi-3 Mini 4K* | 3B | 4k tokens |
| *Phi-3 Medium 14B 128K* | 14B | 128k tokens |
| *Phi-3 Medium 14B 4K* | 14B | 4k tokens |
| *Phi-2* | N/A | 4k tokens |

### Qwen (Qwen)

---

| **Model Name** | **Parameters** | **Context Window** |
| --- | --- | --- |
| *Qwen QwQ Preview 32B* | 32B | 32k tokens |
| --- | --- | --- |
| *Qwen 2.5 Coder 32B* | 32B | 128k tokens |
| *Qwen 2.5 Coder 14B* | 14B | 32k tokens |
| *Qwen 2.5 Coder 7B* | 7B | 128k tokens |
| *Qwen 2.5 Coder 3B* | 3B | 32k tokens |
| *Qwen 2.5 Coder 1.5B* | 1.5B | 128k tokens |
| *Qwen 2.5 Coder 0.5B* | 0.5B | 32k tokens |

### StarCoder (StarCoder)

---

| **Model Name** | **Parameters** | **Context Window** |
| --- | --- | --- |
| *StarCode 2 15B* | 15b | 16k tokens |
| --- | --- | --- |

Updated on March 5, 2025, 9:13 PM UTC

---

[Manual Installation](/products/core-dependencies/ollama/manual-installation)

[Troubleshooting](/products/core-dependencies/ollama/troubleshooting)

---

# Pieces for Developers

# Pieces for Developers

---

Ollama | Troubleshooting
========================

Read documentation on basic Ollama troubleshooting steps below if you’re experiencing connectivity issues between the Ollama client and PiecesOS.

You can also find minimum and recommended device specifications for running local LLMs below.

---

Initial Checks
--------------

PiecesOS uses Ollama as a wrapper for running local large language models, which allows developers to experiment with local models while leveraging PiecesOS features.

However, issues may arise from configuration mismatches, dependency updates, or environmental factors.

For starters, make sure your machine meets the minimum system specifications and you have the latest version of PiecesOS and Ollama.

* **System Requirements:** Confirm your machine meets the minimum hardware and software requirements for running Ollama and your selected language model.
* **Latest Version:** Make sure you are using the latest version of both PiecesOS and Ollama.

Simple updates often resolve known bugs.

### Common Issues & Fixes

If Ollama fails to install or update, or if you encounter any dependency errors during installation, try the following steps to resolve the issue(s):

* **Verify Dependencies:** Ensure all required libraries and tools are installed on your system. Refer to the installation guide for the list of prerequisites.
* **Reinstall:** Uninstall any previous version of Ollama and perform a clean installation.
* **Network Issues:** If downloading fails, check your internet connection or proxy settings.
* **Error Messages:** Look for specific error messages during installation and consult the [Ollama FAQ](https://github.com/ollama/ollama/blob/main/docs/faq.md) or official installation troubleshooting docs for guidance.

### Performance Problems

When running PiecesOS and using local models for generative AI use, you may experience slow response times or high CPU and RAM usage that causes your overall system to slow down.

You may need to:

* **Switch to Cloud Models:** You may need to acquire or switch over to a device with more RAM, a newer CPU with more cores, or a dedicated GPU with more VRAM.
* **Use a Lighter Model:** You may need to use a parameterized or more lightweight model if available.
* **Cut Down Instances:** If you are running multiple instances or flooding the LLM with queries and prompts, reduce the number of concurrent processes.

Using Local Models
------------------

Older devices, regardless of operating system, may struggle to meet the hardware demands of heavy-duty LLMs, even with Ollama's streamlined setup.

### Minimum System Requirements

Local models demand more from your system than their cloud-hosted counterparts.

To ensure a stable, responsive experience—make sure your device fits these general minimum device specifications, pulled from Ollama documentation and other experience-tested public sources.

1

### Operating System

Ollama is supported on macOS, Windows, and Linux devices—but you need to make sure your operating system is running at the correct minimum version to avoid compatibility issues.

* **macOS:** macOS 11.0 (Big Sur) or later.
* **Windows:** Windows 10 or later.
* **Linux:** Ubuntu 18.04 or later.

2

### RAM

Your system should have a minimum amount of RAM depending on the local model you’re trying to run.

More RAM may further improve performance and reduce bottlenecks.

* **3B Models:** 8GB of RAM
* **7B Models:** 16GB of RAM
* **13B Models:** 32GB of RAM

3

### CPU

If your system doesn’t have a dedicated or otherwise capable GPU, running a CPU-tuned model may be in your best interests.

* **Recommended:** Any modern CPU with at least 4 cores.
* **13B Models:** Any modern CPU with at least 8 cores.

4

### GPU

While you don’t need a GPU to run a local Ollama model as long as the LLM is CPU-tuned, a GPU can significantly speed up inference and the training of custom models.

* **Recommended:** Any modern GPU with at least 6GB of VRAM.

5

### Storage Space

Local large language models can occupy significant disk space, so ensure you have enough capacity for both the core installation and any custom models you plan to download or train.

* **Minimum:** At least 12GB of free storage space for installing Ollama and other base models.
* **Additional Storage:** Required for larger models that have additional dependencies

### Choosing the Right Model

Pick a model that fits your system's capabilities and performance limits, especially if you're using an older or less powerful device.

* **Lightweight Models:** Choose smaller or **quantized models** if you're using older hardware or have limited VRAM. Quantized models use less memory, making them easier to run without greatly affecting output quality for general tasks.
* **GPU-Tuned Models:** If you have a powerful GPU with enough VRAM, GPU-accelerated models usually run faster and deliver results more efficiently.
* **CPU-Tuned Models:** If you don't have a dedicated GPU or have low GPU memory, CPU-tuned models are a good alternative. Though slower, they can still offer reliable performance.

### Local Model Crashing

If you encounter 'hanging' or crashing issues when trying to run Pieces with a local LLM, it might be due to your system's hardware.

Insufficient system resources like RAM or VRAM can cause hiccups, slowdowns, and other glitches.

Here are some troubleshooting options:

1. **Check Hardware:** Ensure you have enough RAM, VRAM, and CPU capacity as recommended by the model.
2. **Update Drivers:** Use `vulkaninfo` (or a similar tool) to check for GPU or Vulkan-related errors if you have a Vulkan-based GPU. Update your GPU drivers if you find compatibility issues.
3. **Model Switching:** If you experience crashes or slowdowns, try switching to a less resource-intensive local model. Reducing complexity can help stabilize performance.

If you've tried all these troubleshooting steps but still face crashes, hanging, or other instabilities, you may need to switch to a cloud-based LLM.

### Vulkan-based GPUs

NVIDIA and AMD both use the Vulkan API framework in their GPUs, but there are known issues when using Vulkan GPUs for AI and LLM-focused tasks. For instance, a corrupted or outdated Vulkan API can lead to crashes.

If you encounter this problem, check Vulkan health in your terminal or command line by scanning for errors or warning messages. If any issues are found, **update your GPU drivers.**

### Checking Vulkan

To check your Vulkan health status, run `vulkaninfo` in your terminal or command line and look for errors or warnings.

### Updating GPU Drivers

If issues are detected, update your GPU drivers to ensure Vulkan compatibility and stability.

Updated on March 5, 2025, 9:13 PM UTC

---

[Supported Models](/products/core-dependencies/ollama/supported-models)

[JetBrains Plugin](/products/extensions-plugins/jetbrains)

---

# Pieces for Developers

# Pieces for Developers

---

Understanding Ollama
====================

Read about Ollama, an optional dependency which enables fully on-device generative AI capabilities between [Pieces Drive](https://beta.docs.pieces.app/products/core-dependencies/pieces-os#pieces-drive) and the [Pieces Copilot](https://beta.docs.pieces.app/products/core-dependencies/pieces-os#pieces-copilot).

---

What Is Ollama?
---------------

Ollama is an *optional* but powerful dependency that allows PiecesOS to run Large Language Models (LLMs) directly on your device instead of relying on cloud-based AI processing.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/figma_mockups/ollama_core_dependencies.png)

Unlike PiecesOS—which is required for all Pieces functionality—Ollama is only needed if you want to use local AI models with Pieces Copilot.

---

Using Ollama for local generative AI is *a separate process* from how PiecesOS uses local, in-house LoRA models for processing data with the LTM-2 Engine.

---

### Why Use Ollama?

Ollama acts as a wrapper around `llama.cpp`, making local AI processing faster, more stable, and easier to use.

It simplifies model deployment and ensures compatibility with [Pieces for Developers’ supported LLMs](https://beta.docs.pieces.app/products/core-dependencies/ollama/supported-models) and allows for new local models to be integrated soon after they are released.

Additionally, many developers and organizations prefer local LLMs over cloud-hosted models for reasons such as:

* **Stronger data security,** as it keeps proprietary code and sensitive queries 100% local.
* **Faster response times** with no network delays during generative AI processing and local inference.
* **Offline accessibility** for use even when no internet connection is present.
* **Enterprise compliance** so all AI queries remain within company-managed environments.

### How It Works

Ollama is integrated with PiecesOS to enable local modal inference and generative AI capabilities.

Here’s how Ollama works alongside PiecesOS:

* **Serves on-device LLMs**, reducing cloud dependency and enhancing privacy.
* **Automatically detects existing Ollama installations**, so if you’ve already installed it for other purposes, Pieces can use it without requiring a reinstallation.
* **Supports a curated set of models**, all using `Q4_K_S` quantization for performance optimization.
* **Ensures compatibility** with PiecesOS through minimum version requirements to avoid stability issues.

Since Ollama is several GBs in size, it is **not installed by default**. Users must opt in during the PiecesOS setup process if they want local model support.

### Using Local vs Cloud Models

PiecesOS primarily uses cloud-based AI models for Pieces Copilot.

However, users who prefer on-device AI for speed, privacy, or offline access can install Ollama to enable local models.

[You can read about the system requirements for Ollama here.](https://beta.docs.pieces.app/products/core-dependencies/ollama/troubleshooting#using-local-models)

---

| **Feature** | **Cloud AI (Default)** | **Local AI (Ollama)** |
| --- | --- | --- |
| **Processing Location** | Cloud-based (requires internet) | On-device (runs locally) |
| --- | --- | --- |
| **Performance** | Dependent on internet speed | Potentially faster response times (no network latency) |
| **Data Privacy** | Data only sent to cloud if included as context in a Copilot Chat—reverts on OpenAI, Gemini Privacy Policies | 100% local (no data transmission from local device) |
| **Model Availability** | Uses several cloud-hosted models | Uses user-installed local models |
| **Storage Requirements** | Minimal outside of the PiecesOS installation | Several GBs (model storage) |
| **Offline Support** | No | Yes |

---

Updated on March 5, 2025, 9:13 PM UTC

---

[Troubleshooting](/products/core-dependencies/pieces-os/troubleshooting)

[Manual Installation](/products/core-dependencies/ollama/manual-installation)

---

# Pieces for Developers

# Pieces for Developers

---

PiecesOS | Manual Installation
==============================

Find standalone download links for PieceOS that match your system’s operating system (OS) or architecture, or download PiecesOS automatically with the [Pieces Desktop App.](https://beta.docs.pieces.app/products/desktop-application)

---

Manual Installation Guide
-------------------------

To get started, you’ll need to make sure your device is compatible with Pieces and has the minimum device recommendations.

### System Requirements

For macOS, Windows, and Linux devices, we recommend meeting these minimum device specifications:

---

| **Minimum** | **Recommended** | **Additional Information** |
| --- | --- | --- |
| Any modern CPU | A multi-core CPU | PiecesOS supports multithreading for different operations and processes, making it faster the more CPU cores it can use. |
| --- | --- | --- |
| 8GB of RAM | 16GB+ of RAM | At least 1GB of free RAM if running in cloud mode, or 2GB if running in local mode. |
| 6GB of storage space | 10GB+ of storage space | At least 2GB for PiecesOS and the Pieces Desktop App with at least 4GB of free space for LTM data. |

---

PiecesOS and the Pieces Desktop App are lightweight, but you want to ensure you’ve got at least the above specifications to ensure a seamless user experience.

### Checking OS Version

PiecesOS is designed for macOS, Windows, and Linux, so you have the power of Pieces are your fingertips regardless of your device’s operating system.

Still, you need to be running a minimum version of that OS to ensure compatibility.

---

| **macOS** | **Windows** | **Linux** |
| --- | --- | --- |
| macOS 12.0 (Monterey) or higher | Windows 10 (v.1809) or higher | Ubuntu 22+ |
| --- | --- | --- |

---

For Linux users, please note that PiecesOS is tested and currently compatible with Ubuntu 22+ and other Ubuntu-based distributions **only.**

If you need help determining your device’s OS version or system specifications, [read this documentation on the Troubleshooting page.](https://beta.docs.pieces.app/products/core-dependencies/pieces-os/troubleshooting#common-installation-issues)

### Why Install PiecesOS Manually?

PiecesOS is installed by default when installing the Pieces Desktop App as a core dependency, but it can be installed manually for use with a Pieces plugin or extension if you’re not ready to dive into the entire Pieces ecosystem.

LTM, Pieces Drive, and Pieces Copilot will still be available to you.

### Installing via Pieces Desktop App

If you want to install PiecesOS automatically alongside the Pieces Desktop App, you can check out [these installation guides.](https://beta.docs.pieces.app/products/meet-pieces/)

Manual Download & Installation
------------------------------

There are different ways to install PiecesOS as a standalone entity, separate from the Pieces Desktop App.

To install, download the appropriate link for your device’s OS and architecture (if using a macOS device) and open the file, then walk through the guided installation steps as necessary.

---

macOS

Windows

Linux

**Apple Devices**

---

### Uninstalling PiecesOS

Select your operating system below to platform-specific steps to uninstalling PiecesOS.

---

macOS

Windows

Linux

**On macOS Devices**

On your macOS device, navigate to **Finder,** then select **Applications.**

Scroll or search until you find `PiecesOS.` Right-click on these two applications and select `Move to Trash`.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/meet_pieces_assets/meet_pieces/get_started/macos/macos_how_to_uninstall_pfd.gif)

---

Troubleshooting
---------------

If you’re experiencing issues related to the manual installation of PiecesOS, see the [troubleshooting guide](https://beta.docs.pieces.app/products/core-dependencies/pieces-os/troubleshooting#piecesos--troubleshooting) for quick fixes or access the [support page](https://beta.docs.pieces.app/products/support) for helpful resources.

Want to install PiecesOS automatically with the Pieces Desktop App? [Click here.](https://beta.docs.pieces.app/products/core-dependencies/pieces-os/manual-installation#installing-via-pieces-desktop-app)

Updated on March 5, 2025, 9:13 PM UTC

---

[PiecesOS](/products/core-dependencies/pieces-os)

[Quick Menu](/products/core-dependencies/pieces-os/quick-menu)

---

# Pieces for Developers

# Pieces for Developers

---

PiecesOS | Quick Menu
=====================

Read documentation and see visuals demonstrations of the PiecesOS Quick Menu for macOS, Windows, and Linux.

---

Overview
--------

The **Quick Menu** is a lightweight interface for interacting with PiecesOS, located in your menu bar (macOS & Linux) or system tray (Windows).

This popover lets manage your PiecesOS settings, Long-Term Memory (LTM-2) Engine configurations, and application integrations.

It enables you to monitor PiecesOS status, toggle memory and AI processing settings, and access critical resources without launching a separate application, and more.

[For Linux users, available options are currently more limited.](https://beta.docs.pieces.app/products/core-dependencies/pieces-os/quick-menu#linux)

Quick Menu Actions
------------------

There are several views and buttons you can click to expand or enter in the PiecesOS Quick Menu on macOS and Windows:

---

| **View / Button** | **Explanation** |
| --- | --- |
| *Account* | You can log in or log out of your Pieces Account. |
| --- | --- |
| *Version* | Check for and automatically install available PiecesOS updates. |
| *Long-Term Memory Engine* | Enable or disable LTM. |
| *Long-Term Memory Access Control* | Enable or disable sources from which LTM captures contextual workflow data. |
| *Settings* | Adjust settings like launch on login, enabled Pieces products, ML processing configurations, telemetry permissions, or optimize RAM usage. |
| *Activity* | View recent LTM activity, like most recent Copilot Chats, people, and related websites. |
| *Resources* | Find links to documentation and Pieces for Developers social accounts. |

---

*PiecesOS Quick Menu on macOS*

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_quickmenu_new.png)

---

The PiecesOS Quick Menu shares the same user interface (UI) as the Quick Menu on Windows.

Media for this Quick Menu documentation has been **captured exclusively on macOS.**

---

### Logging In & Out

At the top of the Quick Menu, you can **log in or log out** of your Pieces account. Logging in allows access to cloud-connected features, such as:

* Syncing preferences across devices
* Cloud model processing (if enabled)
* Accessing saved snippets across devices

If you log out, PiecesOS will continue running locally with saved configurations but will not sync to the cloud.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_account_menu.png)

### Checking for Updates

Directly under your account information, the Quick Menu displays your current PiecesOS version and whether it is up to date.

* If a new update is available, you will see an `Update Available` button. Clicking it will initiate the update process.
* If PiecesOS is up to date, the menu will display a green check-mark and show the active version—i.e., 11.0.4.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_checking_for_updates.png)

### LTM-2 Engine

LTM is PiecesOS’s core memory engine, capturing workflow context locally to enhance productivity. You can toggle it on or off from the Quick Menu.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_enable_ltm.gif)

### Long-Term Memory Access Control

This section allows you to manage which applications and sources LTM captures data from through two menus:

1. **Enabled Sources**

This view displays a list of apps that LTM is actively gathering data from (e.g., Google Chrome, ChatGPT).

Clicking on an enabled source opens a window where you can choose to disable it.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_view_enabled_sources.gif)

2. **Disabled Sources**

Apps or sources that LTM is blocked from accessing appear here.

Click the `Add` button to open a Finder (macOS) or Folder (Windows) window, where you can manually select additional applications to allow memory capture.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_disable_a_ltm_source.gif)

Settings
--------

The `Settings` menu allows you to configure PiecesOS to best suit your workflow.

### Optimize Memory Usage

Adjusts memory allocation for PiecesOS to reduce resource consumption while maintaining performance.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_settings_page.png)

### Launch on Login

You can enable or disable automatic startup for PiecesOS when your computer boots up.

### Enabled Apps

View a list of applications that have Pieces plugins and extensions installed.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_enabled_apps.png)

### Telemetry Sharing

Enable or disable telemetry data collection on an individual application, plugin, or extension basis.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_telemetry_settings.png)

### ML Processing

This menu controls how PiecesOS manages AI model inference. You can use `Blended Mode` or `Local Mode` to switch between using a combination of local and cloud-based LLMs for performance, or purely on-device processing with no cloud dependencies.

You can also make changes to the enrichment to level and type to individual applications for more fine-tuned control.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_choose_blended_or_local_per_app_ltm_enrichment.png)

### Activity

The `Activity` menu provides a log of recent actions and interactions within PiecesOS, such as:

1. **Most Recent Copilot Chat:** Displays the last Copilot conversation timestamp.
2. **Most Recent People:** Lists recognized people based on saved materials and shared code.

   **Most Recent Websites:** Shows the last visited web pages that were captured by PiecesOS. Each website is listed with a clickable URL

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_activity_menu.png)

### Resources

The `Resources` provides quick access to documentation, support, and community links:

Links

Follow Us

* [About](https://pieces.app/about)
* [Documentation](https://docs.pieces.app/)
* [Support](https://docs.pieces.app/support)

---

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/macos_resources.png)

Linux
-----

The Pieces Quick Menu on Linux contains several helpful resources and links, such as:

* **Discoverer Integrations:** Opens the Pieces website where all [Pieces for Developers browser and IDE extensions and plugins](https://pieces.app/plugins) can be found.
* **About:** Launches the [About](https://pieces.app/about) page for Pieces for Developers.
* **Documentation:** Brings you to the official [Pieces for Developers documentation](https://docs.pieces.app/), where you can browse through guides and troubleshooting information for the entire Pieces Suite.
* **Submit Feedback or Issues:** If you are experiencing an issue, click this button to open a webpage containing a type form for submitting bug reports or providing feedback—[or visit our Support page here to do so.](https://docs.pieces.app/support)
* **Optimize Memory Usage:** Adjusts memory allocation for PiecesOS to reduce resource consumption while maintaining performance.
* **Quit:** Quits PiecesOS. To relaunch, open your terminal and type `pieces-os` and press `enter` or find the PiecesOS launcher from your application tray.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/pieces_os_main/quick_menu/linux_quick_menu.png)

As a background resource task, there are currently less adjustable actions, but LTM Pieces Drive and Pieces Copilot can still be accessed and utilized within Pieces plugins and extensions on Linux.

Updated on March 5, 2025, 9:13 PM UTC

---

[Manual Installation](/products/core-dependencies/pieces-os/manual-installation)

[Troubleshooting](/products/core-dependencies/pieces-os/troubleshooting)

---

# Pieces for Developers

# Pieces for Developers

---

PiecesOS | Troubleshooting
==========================

Read about what actionable troubleshooting steps to take if PiecesOS isn’t working as expected on your macOS, Windows, or Linux device.

---

Common Installation Issues
--------------------------

Navigate between the grouped tabs below to find information on several common installation issues users experience when downloading and installing PiecesOS.

### Checking System Specifications

Navigate between the grouped tabs below to find information on several common installation issues users experience when downloading and installing PiecesOS.

macOS

Windows

Linux

**macOS | Checking System Specifications**

It is imperative to install the right version of PieceOS depending on whether your chip is ARM or Intel-powered.

Intel and Apple Silicon (ARM) devices run on entirely separate CPU architectures, so downloading the wrong package for your Apple device can cause apps from the Pieces Suite to be rendered useless.

To determine what CPU architecture your Apple device utilizes:

1. Click the **Apple () icon** in the top-left corner of your screen.
2. Select **About This Mac**, and look for the **Overview section.** The first line will contain your CPU type:

* **Apple Silicon / ARM:** You will see an M-Series processor, e.g., `Apple M3`
* **Intel:** You will see an Intel processor, e.g., `2.6 GHz Intel Core i7`

![](https://storage.googleapis.com/hashnode_product_documentation_assets/meet_pieces_assets/meet_pieces/troubleshooting/macos/macos_checking_about_mac.gif)

Once you’ve determined your CPU architecture, [download the correct PiecesOS file accordingly.](https://beta.docs.pieces.app/products/core-dependencies/pieces-os/manual-installation#piecesos-download-links)

### Versions & Updates

Many issues can stem from out-of-date plugins, extensions, or PiecesOS itself.

macOS

Windows

Linux

**macOS | Updating PiecesOS**

To restart and check for updates to PiecesOS on macOS:

1. Restart PiecesOS
2. Ensure PiecesOS is running (look for the **Pieces Icon** in your system tray)
3. Check for and install available updates

![](https://storage.googleapis.com/hashnode_product_documentation_assets/meet_pieces_assets/meet_pieces/troubleshooting/macos/macos_checking_piecesos_for_updates.gif)

---

### Checking OS Version

Having an out-of-date operating system version can cause a range of installation issues with PiecesOS, including scenarios in which the entire application refuses to run.

macOS

Windows

Linux

**macOS | Checking OS Version**

Pieces Suite applications require a minimum macOS version of **macOS 12.0 (Monterey).** If you’re experiencing installation issues, you should first check your OS version to make sure it’s up-to-date.

To determine your Apple device’s version of macOS:

1. Click the **Apple** icon in the top-left corner of your screen
2. Select `About This Mac`
3. Under your device name (i.e., MacBook Air), look for the last line on the list, titled `macOS`

The `macOS` line will report what version of macOS installed, e.g., `Sequoia 15.1.1`.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/meet_pieces_assets/meet_pieces/troubleshooting/macos/macos_checking_about_mac.gif)

---

### Contact Support

If you’ve ensured your device meets the recommended *and* minimum specifications, you’ve performed a clean installation, updated your OS and checked PiecesOS for updates and you’re still experiencing issues, [visit our support page for more resources.](https://beta.docs.pieces.app/products/support)

Updated on March 5, 2025, 9:13 PM UTC

---

[Quick Menu](/products/core-dependencies/pieces-os/quick-menu)

[Ollama](/products/core-dependencies/ollama)

---

# Pieces for Developers

# Pieces for Developers

---

Understanding PiecesOS
======================

Read about PiecesOS, the foundational layer that supports the whole Pieces for Developers Suite, and the core functionalities—like LTM-2, Pieces Drive, and Pieces Copilot.

---

What is PiecesOS?
-----------------

PiecesOS is a background services that runs on your machine, orchestrating local data processing and managing house-made machine learning (ML) models used within Pieces software.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/figma_mockups/longtermmemory_piecesdrive_piecescopilot.png)

There are three core pillars of Pieces functionality powered by PiecesOS: [[1] LTM-2,](https://beta.docs.pieces.app/products/core-dependencies/pieces-os#ltm-2) [[2] Pieces Drive,](https://beta.docs.pieces.app/products/core-dependencies/pieces-os#pieces-drive) and [[3] Pieces Copilot.](https://beta.docs.pieces.app/products/core-dependencies/pieces-os#pieces-copilot)

These components make up the Pieces experience and are essential to the modern AI-powered workflow.

### The Role of PiecesOS

PiecesOS provides the intelligence and power behind Pieces software in two key ways: by (1) supplying the essential components of the Pieces infrastructure and supporting various processes, and (2) powering standalone Pieces plugins and extensions when used without the Pieces Desktop App.

This ‘brain’ is required to enable the 3 fundamental qualities of the Pieces development experience.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/figma_mockups/piecesos_bridging_all_products.png)

### How Does PiecesOS Work?

Powered by PiecesOS, the Long-Term Memory (LTM-2) Engine tracks your development and workflow context using the Pieces Drive, allowing you to import and export materials as needed.

Then, with Pieces Copilot, you can use cloud-based or local LLMs through Ollama to start chats that include the context you need.

This way, all AI-powered elements work together to enhance your context and creativity, especially when you need a memory boost.

Fundamental Components
----------------------

Using the Pieces Desktop App or a Pieces plugin or extension for your favorite IDE requires PiecesOS, as it is a required dependency for memory and context preservation, storing and accessing materials, and interacting with generative AI.

LTM-2
-----

The **Long-Term Memory (LTM-2) Engine** is a powerful evolution of the original LTM system, designed to store and surface workflow context from up to **nine months** in the past.

By combining automatic “roll-ups” with flexible “memories” browsing, LTM-2 ensures you don’t lose track of code, discussions, or references—even if you return to a project weeks or months later.

This can be found within the [Workstream Activities](https://beta.docs.pieces.app/products/desktop-application/workstream-activity) view in the [Pieces Desktop App](https://beta.docs.pieces.app/products/desktop-application).

---

*Pieces Copilot with LTM-2 Context —* Pieces Desktop App

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734028488469/d128a760-7e91-4dbf-8cfc-e7fc257488b7.png?auto=compress,format&format=webp&q=75?auto=compress,format&format=webp&q=75)

---

### Grounded Assistance

Long-Term Memory designed to boost developer productivity by providing assistance that’s temporally-grounded within the concrete context of your actual workflow, allowing the Pieces Copilot to better understand your development process over time.

This helps it offer more relevant and timely responses to queries, as it has a *local database* of information to work from.

Since LTM has a local database of information to work from, it can offer **relevant, timely** responses to your queries:

* Recall details from older tasks or code reviews without requiring you to re-describe them.
* Understand ongoing projects more holistically, anticipating next steps and offering suggestions aligned with your actual workflow.

### How Context is Captured

Under the hood, LTM monitors your workflow at the operating system level, capturing data from:

* **IDEs** (e.g., changes, commits, open files)
* **Browsers** (e.g., opened tabs, reference links)
* **Collaboration tools** (e.g., messaging apps, file-sharing platforms)

LTM (through PiecesOS) extends your ability to [enable or disable specific sources](https://beta.docs.pieces.app/products/core-dependencies/pieces-os/quick-menu#quick-menu-actions) for data capture—this way, you can decide exactly what gets tracked and stored, providing flexibility if you have sensitive or personal workflows.

---

The data captured by LTM is processed and stored entirely on your device, so you don’t need to worry about privacy and security—[which you can read more about here.](https://beta.docs.pieces.app/products/privacy-security-your-data)

---

### Less Context Switching

Traditionally, AI tools require you to restate your environment—what project you’re working on, the code you just wrote, or the documentation you’ve referenced.

However, with LTM-2’s temporal grounding, you have:

* **Reduced Manual Input**: No more duplicating the same context or re-pasting large code blocks.
* **Enhanced Continuity**: Your interactions flow seamlessly from one coding session to the next, letting you pick up exactly where you left off.
* **Intuitive Interactions**: Ask questions like “What was I working on with John last week?” or “How do I fix that same CocoaPods issue from last Tuesday?”

Pieces Copilot can reference stored LTM data to give you real-time, **context-aware** answers.

### Workstream Activity

In the [Pieces Desktop App](https://beta.docs.pieces.app/products/desktop-application), the 2nd-generation LTM comes with a incredibly powerful feature called [Workstream Activity](https://beta.docs.pieces.app/products/desktop-application/workstream-activity).

The Workstream Activity view is a dedicated interface in the Pieces Desktop App that provides a continuous snapshot of your recent tasks, discussions, and code or document reviews—captured by the **Long-Term Memory (LTM-2)** **Engine.**

Every 10 minutes, LTM generates a *roll-up* that summarizes your workflow context for that period, highlighting details such as major tasks, key decisions, and follow-up actions.

---

*Reviewing a LTM Roll-up —* Pieces Desktop App

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1740758250747/b98dc42a-21f8-4282-b45b-4b1d7afe8573.png?auto=compress,format&format=webp&q=75)

---

In addition to offering a timeline of roll-ups, the Workstream Activity view [lets you search for keywords, open references or links, and even launch Pieces Copilot Chats](https://beta.docs.pieces.app/products/desktop-application/workstream-activity#interacting-with-roll-ups) directly from a summarized entry.

By surfacing relevant data right when you need it, Workstream Activity eliminates the frustration of recounting your background or re-pasting massive prompts every time you want AI assistance—empowering you to focus on high-value work instead.

---

This means you can revisit precisely what you worked on in the past, even if you step away from a project for *weeks* or *months.*

---

### On-Device Data Storage

All data captured by the LTM is stored locally on your device. At no point does this data leave your device or become accessible to anyone, including the Pieces team, unless you choose to share it.

LTM applies on-device machine learning algorithms to filter out sensitive information and secrets, maintaining high levels of performance, security, and privacy.

For advanced components that require blended processing, user preferences can be set to leverage a cloud-powered Large Language Model as the Copilot’s runtime.

Pieces Drive
------------

Pieces Drive is an intelligent and interactive material manager baked into PiecesOS that allows you to save, manage, and share important developer resources such as notes, useful code snippets, links, and more within your personal local repository.

From this repository, you can view, edit, reuse, and share these materials via Pieces Shareable Links.

---

*Viewing Saved Materials with Pieces Drive —* Pieces Desktop App

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734026181487/e947ec50-4346-49f1-a967-62b218aafc47.png?auto=compress,format&format=webp&q=75?auto=compress,format&format=webp&q=75)

---

### Multi-Tool Integration

With PiecesOS, you can use Pieces Drive in tools like [Obsidian](https://beta.docs.pieces.app/products/obsidian), [Raycast](https://beta.docs.pieces.app/products/raycast), [Microsoft Teams](https://beta.docs.pieces.app/products/microsoft-teams), and your favorite code editors like [IntelliJ](https://beta.docs.pieces.app/products/extensions-plugins/jetbrains), [Visual Studio Code](https://beta.docs.pieces.app/products/extensions-plugins/visual-studio-code), and [Sublime Text](https://beta.docs.pieces.app/products/extensions-plugins/sublime), and *much more.*

You can also view and adjust metadata saved with your materials in the Applet View of Pieces plugins or the Pieces Desktop App.

### AI-Powered Enrichment

When you save code or other useful materials to Pieces Drive, AI automatically enriches them with a wide variety of useful metadata.

The enrichment process, powered by on-device ML models, provides:

* annotations
* anchors
* shareable links
* related links
* related people
* suggested searches
* tags
* sensitive information
* date created
* source of origin

---

Pieces will also identify and flag any sensitive information, like API keys or personal data, and flag this information under **Sensitive Information.**

---

Pieces Copilot
--------------

Like Pieces Drive, you can find Pieces Copilot nested within Pieces plugins and extensions or the Pieces Desktop App, where it can be used to generate code, answer questions, add code documentation, and serves as the bridge between you and your workflow context as captured by the LTM-2.

---

*Pieces Copilot for Code Generation —* Pieces Desktop App

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734028533289/f37f7075-f2d2-4a99-b948-c23fd70e7257.png?auto=compress,format&format=webp&q=75?auto=compress,format&format=webp&q=75)

---

### Introducing Context to Chats

If you encounter a scenario where you need to recall a specific URL to a Firestore database that was used during a live demo with another coworker, you can use Pieces Copilot (with LTM enabled) to recall that *exact link.*

### Understanding Code

Pieces Copilot helps you comprehend complex code snippets by providing explanations, detailing how specific functions work, and suggesting alternative implementations for that code.

Try asking it alternative ways to handle logging, or more robust methods of error handling—give it an entire file, an entire *project,* or just a function.

### Code Generation

By specifying your requirements for a function or script, Pieces Copilot can generate code that fits into your active file seamlessly.

Use Pieces Copilot and Pieces Drive together to generate boilerplate code or adjust a function, then save it to Pieces Drive with enriched metadata for future use, saving time.

### Error Handling

Debugging, checking logs, and handling errors are often the toughest parts of coding. But with Pieces Copilot, you can debug code thoroughly using the Pieces Desktop App or your preferred plugin.

Context-based debugging is powerful because generative AI, whether cloud-based or local, can understand code semantics beyond what's visible and within the project's scope to find solutions and fixes.

You might even be able to finally center that `div`—seriously, we’ve actually done it.

Updated on March 5, 2025, 9:13 PM UTC

---

[Core Dependencies](/products/core-dependencies)

[Manual Installation](/products/core-dependencies/pieces-os/manual-installation)

---

# Pieces for Developers

# Pieces for Developers

Core Dependencies
=================

---

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/figma_mockups/core_dependencies.png)

---

Learn about [PiecesOS](https://beta.docs.pieces.app/products/core-dependencies/pieces-os) and [Ollama](https://beta.docs.pieces.app/products/core-dependencies/ollama), the two core dependencies that power the Pieces Desktop App and the entire Pieces for Developers suite of [plugins and extensions.](https://beta.docs.pieces.app/products/extensions-plugins)

---

What Are Core Dependencies?
---------------------------

Pieces for Developers products, including the [Pieces for Developers Desktop Application](https://beta.docs.pieces.app/products/desktop-application), utilize *two core dependencies* to provide a local, secure, and efficient development experience—[PiecesOS](https://beta.docs.pieces.app/products/core-dependencies/pieces-os) and [Ollama](https://beta.docs.pieces.app/products/core-dependencies/ollama)**.**

### What Are They?

To run any sort of Pieces software, you will need **[1] PiecesOS,** the backbone of the Pieces Suite. This application is lightweight and runs in the background of your device and powers the [Long-Term Memory (LTM-2) Engine](https://beta.docs.pieces.app/products/core-dependencies/pieces-os#ltm-2), [Pieces Drive,](https://beta.docs.pieces.app/products/desktop-application/managing-materials) and the [Pieces Copilot.](https://beta.docs.pieces.app/products/desktop-application/pieces-copilot)

Running local LLMs requires downloading and installing the **[2] Ollama** wrapper to power on-device AI capabilities, such as querying Pieces Copilot or the local inference required by the LTM-2 Engine.

1. **PiecesOS**: The backbone of the Pieces suite, managing local memory, AI-driven workflow enhancements, and seamless integrations with your development environment.
2. **Ollama**: A specialized wrapper that enables local AI inference, allowing Pieces Copilot and other features to leverage machine learning models *directly on your device.*

### What Do They Do?

These dependencies—**PiecesOS and Ollama**—are lightweight services & engines that handle everything from local model management and context storage to advanced local inference for AI-assisted workflows.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/figma_mockups/pfd_x_piecesos_and_ollama.png)

PiecesOS is **required** for all Pieces products, including:

* Pieces for Developers Desktop App
* Plugins & Extensions for [JetBrains](https://beta.docs.pieces.app/products/extensions-plugins/jetbrains), [VS Code](https://beta.docs.pieces.app/products/extensions-plugins/visual-studio-code), [Sublime Text](https://beta.docs.pieces.app/products/extensions-plugins/sublime), [JupyterLab](https://beta.docs.pieces.app/products/extensions-plugins/jupyterlab), [Azure Data Studio](https://beta.docs.pieces.app/products/extensions-plugins/azure-data-studio), [Neovim](https://beta.docs.pieces.app/products/extensions-plugins/neovim-plugin), [Raycast](https://beta.docs.pieces.app/products/raycast), [Microsoft Teams](https://beta.docs.pieces.app/products/microsoft-teams), [Obsidian](https://beta.docs.pieces.app/products/obsidian), [the Pieces CLI](https://beta.docs.pieces.app/products/extensions-plugins/cli), and more.

### Why Do We Need Them?

Pieces for Developers is designed with **speed and efficiency** in mind, so PiecesOS acts as the end-all between different Pieces products to minimize client-side overhead and additional code—while also being secure and highly-configurable.

![](https://storage.googleapis.com/hashnode_product_documentation_assets/core_dependencies_assets/figma_mockups/performance_privacy_flexibility.png)

Our focus on **security and flexibility** is why we’ve introduced the Ollama wrapper for local large language models—users can switch to entirely on-device generative AI, and by offloading most operations locally, the user experience benefits from:

* **Instant AI-powered assistance** without cloud latency.
* **100% local memory storage** with full control over data.
* **Offline functionality**, ensuring a seamless experience even when disconnected from the internet.
* **Lightweight, background operation**, consuming minimal system resources.

However, you don't have to install Ollama if you don't want to use it.

You can choose to install it if you want to use local models, which is especially useful in enterprise settings where strong device security is important.

---

| **Dependency** | **Purpose** | **Required?** |
| --- | --- | --- |
| PiecesOS | Manages memory, developer material storage, and plugin communication. | Yes — this is required for all Pieces products. |
| --- | --- | --- |
| Ollama | Enables locally-powered generative AI queries and model execution. | No — but this is required for local AI inference. |

---

Updated on March 5, 2025, 9:13 PM UTC
